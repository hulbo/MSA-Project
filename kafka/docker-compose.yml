networks:
  kafka_default:
    driver: bridge

services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    networks:
      - kafka_default
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    networks:
      - kafka_default
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.6.1
    container_name: kafka-connect
    networks:
      - kafka_default
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/plugins,/usr/share/confluent-hub-components"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_RETRY_BACKOFF_MS: "5000"
      CONNECT_REQUEST_TIMEOUT_MS: "20000"
      CONNECT_LOG_LEVEL: "INFO"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      errors.retry.timeout: "300000"
      errors.retry.delay: "5000"
      errors.deadletterqueue.topic.name: "failed-records"
      # DB 연결정보를 환경변수로 넘김 (docker-compose 실행 시 OS 환경변수 필요)
      CONNECT_CONFIG_PROVIDERS: env
      CONNECT_CONFIG_PROVIDERS_ENV_CLASS: org.apache.kafka.common.config.provider.EnvVarConfigProvider
      CONNECT_FILE_CONFIG_PATH: "/etc/kafka-connect/configs/secrets.properties"  # 설정파일
      DB_URL: ${OCI_MARIA_DB_URL}
      DB_USER: ${OCI_MARIA_DB_USER}
      DB_PASSWORD: ${OCI_MARIA_DB_PASSWORD}
    volumes:
      - ./kafka-connectors/jdbc:/etc/kafka-connect/plugins/jdbc
      - ./config:/etc/kafka-connect/configs

  kafka-init:
    image: wurstmeister/kafka
    container_name: kafka-init
    networks:
      - kafka_default
    depends_on:
      - kafka
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      # kafka가 완전히 올라오도록 잠시 대기 후 토픽 생성
      sleep 10;
      
      # 메시지를 60초 동안만 유지하고 자동 삭제
      # 메시지를 소비한 후 즉시 삭제 (큐처럼 동작)
      # 로그 파일 크기를 1MB로 설정하여 삭제 속도 최적화
      kafka-topics.sh --create --topic t_sc_orders --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1
      --config retention.ms=600000 --config log.segment.bytes=50000000  
      "

  kafka-connect-init:
    image: curlimages/curl
    container_name: kafka-connect-init
    networks:
      - kafka_default
    depends_on:
      kafka-connect:
        condition: service_healthy
    volumes:
      - ./config:/config
    entrypoint: ["/bin/sh", "-c"]

    command: |
      "
      sleep 15;
      curl -X POST -H \"Content-Type: application/json\" --data @/config/jdbc-sink-orders.json http://kafka-connect:8083/connectors;
      "

volumes:
  kafka_data:
